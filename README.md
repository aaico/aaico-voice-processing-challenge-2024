#### Team

Team name: chAI team

Members:

- Branislava Jankovic - branislava.jankovic@mbzuai.ac.ae
- Kamila Zhumakhanova - kamila.zhumakhanova@mbzuai.ac.ae
- Sabina Jangirova - sabina.jangirova@mbzuai.ac.ae

### Solution description 

Our model is based on the Pre-trained ASTForAudioClassification on AudioSet ("MIT/ast-finetuned-audioset-10-10-0.4593") using the transformers library. This model is expected to be used for extracting features from audio data and classifying them into predefined categories based on the AudioSet ontology.

A PyTorch module CustomModel wraps the pre-trained AST model. This custom model adds three fully connected layers on top of the AST model's output, with ReLU activations for the first two added layers and a sigmoid activation for the final layer. 

The training of the custom model is done on a dataset for a specific classification task (label "GALACTIC OXYGEN", "GALACTIC TEMPERATURE", "GALACTIC BATTERY" as 0, other audio samples as 1). The dataset to train the model was generated by spliting the provided audio by frames. 1 new audio = 1 frame. For simplicity, we labeled the whole frames as the majority sample class. Due to the fact, that the wake phrases are underrepresented in the sample audio, we are giving more weight to the class 0 (wake phrases). 

The code for generating the audio dataset can be found in "dataset.ipynb". 

Run `pip install -r requirements.txt` to install all necessary dependencies.
