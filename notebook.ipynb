{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Code Repos\\AAICO-Voice-Competition\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import pickle\n",
    "\n",
    "import fastdtw\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### PARAMETERS ###########\n",
    "# DO NOT MODIFY\n",
    "# Desired sample rate 16000 Hz\n",
    "sample_rate = 16000\n",
    "# Frame length\n",
    "frame_length = 512\n",
    "\n",
    "# debug flag\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### AUDIO FILE ###########\n",
    "# DO NOT MODIFY\n",
    "# Path to the audio file\n",
    "audio_file = \"audio_aaico_challenge.wav\"\n",
    "\n",
    "# Read the audio file and resample it to the desired sample rate\n",
    "audio_data, current_sample_rate = librosa.load(\n",
    "    audio_file, \n",
    "    sr=sample_rate,\n",
    ")\n",
    "audio_data_int16 = (audio_data * 32767).astype(np.int16) # converting audio data from 32-bit float to 16-bit int\n",
    "number_of_frames = len(audio_data_int16) // frame_length # number of frames if audio split by 512 frame length\n",
    "audio_data_int16 = audio_data_int16[:number_of_frames * frame_length] # trimming audio data into multiples of 512 \n",
    "audio_duration = len(audio_data_int16) / sample_rate # calculating audio duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Engine 3, this is Bravo Team. We've got heavy smoke and flames visible from the Charlie side. We're going for a defensive attack, over. Galactic temperature. This is Bravo Team. Visibility's near zero, lots of heat. We're gonna hit Charlie hard from the exterior, then make entry from a primary search, over. Galactic battery. Galactic Battery. Keep an eye on that roof. Looks sketchy. Engine 3, better charge the hose. Over. Acknowledged. We've got a 2.5 inch line ready for a blitz attack. We'll knock down the bulk of the fire before we go in. Galactic Oxygen. Pass is active and we're staying on the comms. Going in for the attack now. Stay sharp out there. Over. Bravo going in. Over.\n"
     ]
    }
   ],
   "source": [
    "### Loading model\n",
    "device = \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(\"audio_aaico_challenge.wav\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### STREAMING SIMULATION ###########\n",
    "# DO NOT MODIFY\n",
    "results = np.zeros(shape=(3, len(audio_data_int16)), dtype=np.int64)\n",
    "# Detection mask lines are SENT TIME, LABEL, RECEIVE TIME. \n",
    "buffer = queue.Queue()\n",
    "start_event = threading.Event()\n",
    "\n",
    "def label_samples(list_samples_id, labels):\n",
    "    \"\"\"\n",
    "        Receives the index of samples for a frame \n",
    "        and allocates each sameple's label (0, 1) \n",
    "        and receive time.\n",
    "    \"\"\"\n",
    "    receive_time = time.time_ns()\n",
    "    results[1][list_samples_id] = labels\n",
    "    results[2][list_samples_id] = receive_time\n",
    "\n",
    "def notice_send_samples(list_samples_id):\n",
    "    \"\"\"\n",
    "        Receives the index of samples for a frame and \n",
    "        allocates each sample's send time \n",
    "    \"\"\"\n",
    "    send_time = time.time_ns()\n",
    "    results[0][list_samples_id] = send_time\n",
    "\n",
    "def emit_data():\n",
    "    \"\"\"\n",
    "        Each iteration of the loop generate a list of \n",
    "        indices of total length 512. Retreieve the frame\n",
    "        from audio data using the indicies and inputting \n",
    "        into the buffer. \n",
    "\n",
    "        Sends the same set of indicies to \"notice_send_samples\" \n",
    "        to record sent time.\n",
    "    \"\"\" \n",
    "    time.sleep(.5)\n",
    "    print('Start emitting')\n",
    "    start_event.set()\n",
    "    for i in range(0, number_of_frames):\n",
    "        list_samples_id = np.arange(i*frame_length, (i+1)*frame_length)\n",
    "        time.sleep(frame_length / sample_rate) # Simulate real time\n",
    "        frame = audio_data_int16[list_samples_id]\n",
    "        \n",
    "        # if DEBUG:\n",
    "        #     print(list_samples_id)\n",
    "        #     print(frame)\n",
    "\n",
    "        buffer.put(frame)\n",
    "        notice_send_samples(list_samples_id)\n",
    "    print('Stop emitting')\n",
    "\n",
    "def process_data():\n",
    "    \"\"\"\n",
    "        Loop runs while the current frame's processed isn't equal to\n",
    "        total number of frames. \n",
    "        \n",
    "        Retrieves the frame from the buffer and generates the indicies list \n",
    "        for the samples. Generates the labels for the samples, sends both \n",
    "        parameters to the label_samples function to label which samples are commands\n",
    "        and which are not.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    start_event.wait()\n",
    "    print('Start processing')\n",
    "    while i != number_of_frames:\n",
    "        start_time = time.time()\n",
    "        frame = buffer.get() \n",
    "        \n",
    "        result = pipe(frame)\n",
    "        # list_samples_id = np.arange(i*frame_length, (i+1)*frame_length)\n",
    "        # labels = [1 for _ in range(len(list_samples_id))]\n",
    "        # label_samples(list_samples_id, labels)\n",
    "        \n",
    "        i += 1\n",
    "        end_time = time.time()\n",
    "        duration = (end_time - start_time) / 1000 # convert s to ms\n",
    "        \n",
    "        print(\"Processed Frame: {} | Result: {} | Time Taken: {}\".format(i, result[\"text\"], duration))\n",
    "        \n",
    "    print('Stop processing')\n",
    "    # Save the list to a file\n",
    "    with open('results.pkl', 'wb') as file:\n",
    "        pickle.dump(results, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start emitting\n",
      "Start processing\n",
      "Processed Frame: 1 | Result:  Продолжение следует... | Time Taken: 0.011759534120559693\n",
      "Processed Frame: 2 | Result:  Subtitles by the Amara.org community | Time Taken: 0.012067009449005127\n",
      "Processed Frame: 3 | Result:  Thank you. | Time Taken: 0.010975581645965576\n",
      "Processed Frame: 4 | Result:  Thank you. | Time Taken: 0.010955400228500367\n",
      "Processed Frame: 5 | Result:  Thank you. | Time Taken: 0.011103220462799072\n",
      "Stop emitting\n",
      "Processed Frame: 6 | Result:  Thank you. | Time Taken: 0.011428686857223511\n",
      "Processed Frame: 7 | Result:  Thank you. | Time Taken: 0.01116071367263794\n",
      "Processed Frame: 8 | Result:  Takk for ating medietekst. | Time Taken: 0.01226189351081848\n",
      "Processed Frame: 9 | Result:  I'm sorry. | Time Taken: 0.011101377725601196\n"
     ]
    }
   ],
   "source": [
    "time_measurement = []\n",
    "\n",
    "thread_process = threading.Thread(target=process_data)\n",
    "thread_emit = threading.Thread(target=emit_data)\n",
    "\n",
    "thread_process.start()\n",
    "thread_emit.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
